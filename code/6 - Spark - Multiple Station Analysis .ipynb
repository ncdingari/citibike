{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "import json\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, avg, udf, to_timestamp, date_trunc, stddev_pop, mean\n",
    "from pyspark.sql.functions import year, month, hour, dayofweek\n",
    "from pyspark.sql.functions import round, concat, col, lit\n",
    "from pyspark.sql.functions import log1p\n",
    "\n",
    "from pyspark.sql.types import FloatType, StructType, IntegerType, StringType, DoubleType, StructField, TimestampType, DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, Normalizer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import random\n",
    "\n",
    "spark1 = SparkSession.builder.appName(\"SingleStationRF\").getOrCreate()\n",
    "\n",
    "import datetime as dt\n",
    "print(\"modules imported\")\n",
    "\n",
    "randomSeed = 1984\n",
    "\n",
    "pathWeather = \"/users/sajudson/Dropbox/WPI/DS504/project/repository/weather/\"\n",
    "pathData = \"/users/sajudson/Dropbox/WPI/DS504/project/repository/data/\"\n",
    "pathFigure = \"/users/sajudson/Dropbox/WPI/DS504/project/repository/figures/\"\n",
    "\n",
    "#load expriment dictionary\n",
    "with open(pathFigure+\"experiments.json\",\"r\") as f:\n",
    "    experiments = json.load(f)\n",
    "\n",
    "file_type = \"csv\"\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sample Station Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9792239665985107\n"
     ]
    }
   ],
   "source": [
    "#SELECT WHICH CITY TO USE FOR ANALYSIS \n",
    "city = \"NYC\"\n",
    "#city = \"JC\"\n",
    "\n",
    "tripSampleFraction = .20\n",
    "stationSampleFraction = .20\n",
    "\n",
    "sample = str(int(tripSampleFraction*100))\n",
    "\n",
    "\n",
    "t0= time.time()\n",
    "\n",
    "filenameBF = \"citibike\"+city+\"bf3_sample_\"+sample\n",
    "filenameOutput = filenameBF+\".csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "sampleStationDataFeatureSchema = StructType([StructField('datetime', TimestampType(), False),\n",
    "                              StructField('station_id', IntegerType(), False),\n",
    "                              StructField('totalDemand', IntegerType(), False),\n",
    "                              StructField('totalSupply', IntegerType(), False)    \n",
    "                              ])\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "sampleStationData = spark1.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(sampleStationDataFeatureSchema) \\\n",
    "  .load(pathData+filenameOutput)\n",
    "\n",
    "print(time.time()-t0)\n",
    "# this operation takes ~0.27seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing and debugging only\n",
    "#t0= time.time()\n",
    "#sampleStationData.describe().show()\n",
    "#sampleStationData.show()\n",
    "#print(time.time()-t0)\n",
    "\n",
    "# this operation takes ~3.4 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weather Features Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherFeatures = \"NYC\"+'weatherFeatures'\n",
    "weather_file_type = 'csv'\n",
    "weatherFilename = weatherFeatures + \".\"+weather_file_type\n",
    "\n",
    "weatherFeatureSchema = StructType([StructField('temp', DoubleType(), False),\n",
    "                            StructField('humidity', DoubleType(), True),\n",
    "                            StructField('total_precip', DoubleType(), True),\n",
    "                            StructField('cloud_cover', DoubleType(), True),                 \n",
    "                            StructField('datetime', TimestampType(), True)\n",
    "                           ])\n",
    "\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "weatherFeatures = spark1.read.format(weather_file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(weatherFeatureSchema) \\\n",
    "  .load(pathWeather+weatherFilename)\n",
    "\n",
    "# for testing and debugging only\n",
    "#weatherFeatures.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate List of Stations included in Sample Station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 116, 119, 143, 150, 152, 153, 161, 168, 195, 238, 244, 245, 252, 255, 259, 260, 275, 278, 279, 280, 285, 289, 307, 312, 319, 322, 324, 344, 346, 347, 348, 349, 351, 365, 369, 373, 395, 409, 416, 445, 454, 457, 460, 461, 467, 468, 472, 488, 490, 500, 526, 530, 534, 536, 537, 2002, 2003, 2004, 2012, 2017, 3002, 3041, 3049, 3052, 3061, 3073, 3075, 3076, 3077, 3078, 3098, 3102, 3105, 3107, 3113, 3120, 3121, 3123, 3124, 3135, 3140, 3148, 3150, 3158, 3165, 3166, 3175, 3219, 3224, 3241, 3246, 3249, 3255, 3283, 3296, 3306, 3308, 3311, 3312, 3315, 3318, 3322, 3330, 3333, 3338, 3341, 3342, 3344, 3346, 3347, 3348, 3353, 3354, 3357, 3359, 3368, 3369, 3373, 3376, 3383, 3387, 3392, 3393, 3396, 3397, 3407, 3415, 3417, 3424, 3427, 3441, 3447, 3459, 3464, 3467, 3478, 3496, 3497, 3498, 3508, 3510, 3513, 3516, 3522, 3525, 3528, 3536, 3541, 3551, 3559, 3560, 3561, 3563, 3564, 3568, 3570, 3571, 3576, 3580, 3586, 3599, 3601, 3604, 3606, 3613, 3615, 3617, 3620, 3631, 3632, 3644, 3647, 3648, 3654, 3658, 3666, 3671, 3675, 3680]\n"
     ]
    }
   ],
   "source": [
    "sampleStationList = sampleStationData.select(\"station_id\").distinct().orderBy(\"station_id\").rdd.map(lambda row : row[0]).collect() \n",
    "\n",
    "# for testing and debugging only\n",
    "print(sampleStationList)\n",
    "#Expected Output \n",
    "#[3183, 3184, 3188, 3193, 3199, 3200, 3207, 3209, 3213, 3214, 3220, 3225, 3281, 3483]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect sample station list data set\n",
    "\n",
    "inspectStationSample = False\n",
    "if inspectStationSample == True:\n",
    "      \n",
    "    stationRecordAverage =0\n",
    "    stationRecordMax =0\n",
    "    stationRecordMin =999999999\n",
    "    stationRecordCount = []\n",
    "    stationCount = len(sampleStationList)\n",
    "    for i in  range(0,stationCount):\n",
    "        s = sampleStationList[i]            \n",
    "        sdf = sampleStationData.filter(sampleStationData.station_id == s) #.orderBy('datetime')\n",
    "        stationRecordCount.append(sdf.count())\n",
    "        stationRecordAverage = stationRecordAverage + stationRecordCount[i]/stationCount\n",
    "        if stationRecordMax < stationRecordCount[i]:\n",
    "            stationRecordMax =stationRecordCount[i]\n",
    "        if stationRecordMin > stationRecordCount[i]:\n",
    "            stationRecordMin =stationRecordCount[i]\n",
    "        print(\"station_id =\",s, \"records = \", stationRecordCount[i])\n",
    "        print(sdf.show(1))\n",
    "    print(stationRecordAverage)\n",
    "    print(stationRecordAverage/(3*8760))\n",
    "    print(stationRecordMin/(3*8760))\n",
    "    print(stationRecordMax/(3*8760))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Station Level Data Set\n",
    "- Filter based on station selected\n",
    "- Merge with weather data\n",
    "- Fill hours without any trip data (i.e., demand or supply is null) with zeroes\n",
    "- Create date based features in station data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell defines the functions used to:\n",
    "- Create station level data set\n",
    "- Create input column list\n",
    "- Select Regression Method\n",
    "- Create test/train split based on dates\n",
    "- Run model for single station\n",
    "- Plot predicted vd actual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions defined\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "def createStationDataFrame(station,labelLinkFunction='none'):\n",
    "    print(\"station_id =\",station)\n",
    "    bf_station = sampleStationData.filter(sampleStationData.station_id == station)\n",
    "    \n",
    "    #left join includes all intervals in weather file in output - then fill supply and demand nulls with zeroes\n",
    "    #right joing only includes intervals with supply or demand \n",
    "    bf_station = weatherFeatures.join(bf_station, ['datetime'],how = \"left\")\n",
    "    bf_station = bf_station.fillna({'totalDemand':'0','totalSupply':'0'})\n",
    "\n",
    "    #bf_station.show()\n",
    "    print(\"rows in dataframe\",bf_station.count())\n",
    "    print(time.time()-t0)\n",
    "    \n",
    "    # year month and hour are redundent with metblue data fields\n",
    "    bf_station = bf_station.withColumn(\"year\", year(bf_station.datetime).cast(\"integer\"))\n",
    "    bf_station = bf_station.withColumn(\"month\", month(bf_station.datetime).cast(\"integer\"))\n",
    "    \n",
    "    @udf('boolean')\n",
    "    def ifWeekday(dow):\n",
    "        if dow > 5.0: return False\n",
    "        else: return (True)\n",
    "\n",
    "    @udf('boolean')\n",
    "    def ifRain(precip):\n",
    "        if precip > 0.0: return True\n",
    "        else: return (False)\n",
    "   \n",
    "    bf_station = bf_station.withColumn(\"hourOfDay\", hour(bf_station.datetime).cast('integer'))\n",
    "    bf_station = bf_station.withColumn(\"dayOfWeek\", dayofweek(bf_station.datetime).cast(\"double\"))\n",
    "    bf_station = bf_station.na.drop(how=\"any\", subset=['dayOfWeek','hourOfDay'])\n",
    "    bf_station = bf_station.withColumn(\"weekday\", ifWeekday(bf_station.dayOfWeek))\n",
    "    bf_station = bf_station.withColumn(\"raining\", ifRain(bf_station.total_precip))\n",
    "\n",
    "    #Label y\n",
    "    #linkFunction = \"log1p\"\n",
    "    if labelLinkFunction == \"log1p\":\n",
    "        bf_station = bf_station.withColumn(\"label\", log1p(bf_station.totalDemand))\n",
    "    else:\n",
    "        bf_station = bf_station.withColumn(\"label\", bf_station.totalDemand)\n",
    "    print('bf_station created')\n",
    "    return(bf_station)\n",
    "        \n",
    "def createfeatureInputCols(stationDataFrame,ignoreColumnList):\n",
    "    return([x for x in stationDataFrame.columns if x not in ignoreColumnList])\n",
    "\n",
    "\n",
    "def plotPredictedvsActual(predictedData):\n",
    "    tripsActual = predictedData.select(\"label\").collect()\n",
    "    tripsPredicted = predictedData.select(\"prediction\").collect()\n",
    "    \n",
    "    x1=tripsActual\n",
    "    xlabel='Actual Trips'\n",
    "    y1=tripsPredicted\n",
    "    title1='Predicted vs Actual'\n",
    "\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator2 = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator3 = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"explainedVariance\")\n",
    "\n",
    "    \n",
    "    rmse = evaluator.evaluate(predictedData)\n",
    "    rSquared = evaluator2.evaluate(predictedData)\n",
    "    varianceExplained = evaluator2.evaluate(predictedData)\n",
    "    \n",
    "    \n",
    "    note1 = '{0:5s} RSME = {1:<8.2n}   R2 = {2:<5.3n}'.format(method, rmse, rSquared)\n",
    "    note2 = '{0}, normalized = {1}, link function = {2}'.format(featureName, normalize, linkFunction)\n",
    "\n",
    "\n",
    "    y1label='Preducted Trips'\n",
    "    filename=\"RF_yhat_vs_y_\"+method+featureName+str(normalize)+linkFunction\n",
    "    figurepath = pathFigure\n",
    "    figsaveformat = '.png'\n",
    "    colors = ['blue','green','red']\n",
    "    lw_default = 1\n",
    "\n",
    "    xScaleMin = -2\n",
    "    xScaleMax = np.max(x1)*1.05\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "\n",
    "    def ytickformat(x): \n",
    "        return '$%1.0f' % x\n",
    "\n",
    "    ax.scatter(x1,y1,linewidth =lw_default, color = colors[0], alpha = .05)\n",
    "    ax.set_title(title1)\n",
    "    ax.annotate(note1, xy=(0.15,.9), xycoords = \"figure fraction\")\n",
    "    ax.annotate(note2, xy=(0.15,.85), xycoords = \"figure fraction\")\n",
    "    ax.set_ylabel(y1label)\n",
    "    ax.set_xlim(xScaleMin, xScaleMax)\n",
    "    ax.set_ylim(xScaleMin, xScaleMax)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.format_xdata = ytickformat\n",
    "    ax.format_ydata = ytickformat\n",
    "    ax.grid(True)\n",
    "    #save figure as PNG\n",
    "    figfilename = figurepath+filename+figsaveformat\n",
    "    plt.savefig(figfilename, bbox_inches='tight', dpi = (300))\n",
    "    #print(time.time())\n",
    "    plt.show()\n",
    "    return()\n",
    "\n",
    "def selectRegressionMethod(regressionMethodName,featureName):\n",
    "    \n",
    "    if regressionMethodName == \"rf\":\n",
    "        if test == True:\n",
    "            nt = 1\n",
    "        else: nt = 100    \n",
    "        modelParameters = {'featuresCol':featureName,'numTrees':nt,'subsamplingRate':1,'maxDepth':10}\n",
    "        regressionMethod = RandomForestRegressor(featuresCol=modelParameters['featuresCol'], \n",
    "                                                 numTrees = modelParameters['numTrees'], \n",
    "                                                 subsamplingRate = modelParameters['subsamplingRate'],\n",
    "                                                 maxDepth =modelParameters['maxDepth'])\n",
    "                                                 \n",
    "    elif regressionMethodName == \"gbt\":\n",
    "        modelParameters = {'featuresCol':featureName,'maxIter':10}\n",
    "        regressionMethod = GBTRegressor(featuresCol = modelParameters['featuresCol'],\n",
    "                                    maxIter = modelParameters['maxIter'])\n",
    "        \n",
    "    elif regressionMethodName == \"glr\":\n",
    "        modelParameters = {'featuresCol':featureName, 'family':\"poisson\",'link':'log','maxIter':10, 'regParam':0.3}\n",
    "        regressionMethod = GeneralizedLinearRegression(family = modelParameters['family'], \n",
    "                                                       link = modelParameters['link'], \n",
    "                                                       maxIter = modelParameters['maxIter'],\n",
    "                                                       regParam = modelParameters['regParam'])  \n",
    "    else:\n",
    "        print('Invalid regression method')\n",
    "        return()\n",
    "    #print('Regression method selected')\n",
    "    return(regressionMethod,modelParameters)\n",
    "\n",
    "\n",
    "print(\"functions defined\")\n",
    "\n",
    "   \n",
    "    \n",
    "def timeSeriesTestTrain(df,dates):\n",
    "        train = df.where(df.datetime.between(dates['train'][0],dates['train'][1]))\n",
    "        test = df.where(df.datetime.between(dates['test'][0],dates['test'][1]))\n",
    "        return (train,test)\n",
    "    \n",
    "def runModel(regressionMethodName,\n",
    "             stationID,\n",
    "             stationDataFrame, \n",
    "             featureInputCols, \n",
    "             normalize, \n",
    "             splitMethod = 'random'):\n",
    "    print(\"=\"*80)\n",
    "    print('Station:{0}'\n",
    "          .format(stationID))\n",
    "    print('Model:{0}, Normalize:{1}, LinkFunction:{2}, train/test splitMethod:{3}'\n",
    "          .format(regressionMethodName,normalize,labelLinkFunction,splitMethod))\n",
    "    print(featureInputCols)\n",
    "\n",
    "    \n",
    "    oneHot = OneHotEncoderEstimator(inputCols=[\"hourOfDay\", \"dayOfWeek\"],\n",
    "                                 outputCols=[\"hourOfDayVector\", \"dayOfWeekVector\"])\n",
    "    \n",
    "    stationSummaryAll = stationDataFrame.groupBy('station_id').agg(count('label'), sum('label'), avg(\"label\"),stddev_pop(\"label\"))\n",
    "    stationAvg = stationSummaryAll.select('avg(label)').where('station_id' == stationID).collect()\n",
    "    stationSum = stationSummaryAll.select('sum(label)').where('station_id' == stationID).collect()\n",
    "    stationStd = stationSummaryAll.select('stddec_pop(label)').where('station_id' == stationID).collect()\n",
    "    stationNonZeroCount = stationSummaryAll.select('count(label)').where('station_id' == stationID).collect()\n",
    "    stationCount = stationSummaryAll.select('count(label)').where('station_id' == 'None').collect()\n",
    "    \n",
    "    featureInputCols.extend([\"hourOfDayVector\", \"dayOfWeekVector\"])\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=featureInputCols,\n",
    "        outputCol='features')\n",
    "    \n",
    "    if normalize == True:\n",
    "        normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "        featureName = \"normFeatures\"\n",
    "        regressionMethod, regressionModelParameters = selectRegressionMethod('rf',featureName)\n",
    "        pipeline = Pipeline(stages=[oneHot, assembler, normalizer ,regressionMethod])\n",
    "    else:\n",
    "        featureName = \"features\"\n",
    "        regressionMethod, regressionModelParameters = selectRegressionMethod('rf',featureName)\n",
    "        pipeline = Pipeline(stages=[oneHot, assembler ,regressionMethod])\n",
    " \n",
    "    \n",
    "    trainingDates = ['2016-10-01 00:00:00',\n",
    "                 '2017-9-30 23:59:59']\n",
    "    \n",
    "    testDates = ['2017-10-01 00:00:00',\n",
    "             '2017-10-31 23:59:59']\n",
    "    \n",
    "    dates = {'train':trainingDates, 'test':testDates}\n",
    "\n",
    "\n",
    "    \n",
    "    if splitMethod == 'random':\n",
    "         # Split the data into training and test sets (30% held out for testing)\n",
    "        (trainingData, testData) = stationDataFrame.randomSplit([0.6, 0.4])\n",
    "\n",
    "    else:\n",
    "        (trainingData, testData) = timeSeriesTestTrain(stationDataFrame, dates)\n",
    "    \n",
    "    #fit model and make predictions\n",
    "    model = pipeline.fit(trainingData)\n",
    "    predictedData = model.transform(testData)\n",
    "    #predictedData.select(\"prediction\", \"label\", featureName).show(5)\n",
    "    predictedData\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator2 = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator3 = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"explainedVariance\")\n",
    "    \n",
    "    rmse = evaluator.evaluate(predictedData)\n",
    "    rSquared = evaluator2.evaluate(predictedData)\n",
    "    varianceExplained = evaluator2.evaluate(predictedData)\n",
    "    \n",
    "    \n",
    "    print(\"RMSE, R2, and variance explained on test data = {0:6.3f}, {1:6.3f}, {2:6.3f}\"\n",
    "          .format(rmse, rSquared, varianceExplained))\n",
    "    print()\n",
    "    basetime = 1541216769\n",
    "    experimentTimeStamp = int((time.time()-basetime)/6)\n",
    "    experiment = {experimentTimeStamp:{\n",
    "        \"station\":stationID,\n",
    "        'stationNonZeroCount':stationNonZeroCount,\n",
    "        'stationCount':stationCount,\n",
    "        'stationSum':stationSum,\n",
    "        'stationAvg':stationAvg,\n",
    "        'stationStd':stationStd,\n",
    "        'regressionMethodName':regressionMethodName,\n",
    "        'normalize':normalize,\n",
    "        'linkFunctionLabel':labelLinkFunction,\n",
    "        'featureInputCols':featureInputCols,\n",
    "        'rmse':rmse, \n",
    "        'rSquared':rSquared, \n",
    "        'varianceExplained':varianceExplained,\n",
    "        'version':\"Added OneHotEncode for hOD, dOW\",\n",
    "        'trainSplitMethod':splitMethod}}\n",
    "    experiments.update(experiment)\n",
    "    with open(pathFigure+\"experiments.json\",\"w\") as f:\n",
    "        json.dump(experiments, f)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfHyperParameterSerach(numTreesList,subsamplingRateList,maxDepthList):\n",
    "    print('{0:6s} {1:10s} {2:16s} {3:10s} {4:10s} {5:10s}'.format('time','numTrees','subSamplingRate','maxDepth','rmse','r2'))\n",
    "    for n in numTreesList:\n",
    "        for s in subsamplingRateList:\n",
    "            for d in maxDepthList:\n",
    "                \n",
    "                rf = RandomForestRegressor(featuresCol=\"features\", numTrees = n, subsamplingRate = s, maxDepth = d)\n",
    "                pipeline = Pipeline(stages=[assembler, rf])\n",
    "                model = pipeline.fit(trainingData)\n",
    "                predictions = model.transform(testData)\n",
    "                #predictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "                evaluator = RegressionEvaluator(\n",
    "                labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "                evaluator2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "                rmse = evaluator.evaluate(predictions)\n",
    "                rSquared = evaluator2.evaluate(predictions)\n",
    "                deltaT= time.time()-t0\n",
    "                print('{0:6.2n} {1:10n} {2:16.2n}  {3:10n} {4:10.4n}{5:10.4n}'.format(deltaT,n,s,d,rmse, rSquared))\n",
    "\n",
    "                #print('time = {0:6.2n} numTrees= {1:8n} subsamplingRate= {2:8.2n} maxDepth= {3:8n} RMSE= {4:8.4n}'.format(deltaT,n,s,d,rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run single station model\n",
      "station_id = 3075\n",
      "rows in dataframe 26304\n",
      "1.0573129653930664\n",
      "bf_station created\n",
      "================================================================================\n",
      "Station:3075\n",
      "Model:rf, Normalize:False, LinkFunction:none, train/test splitMethod:dates\n",
      "['temp', 'humidity', 'year', 'month', 'hourOfDay', 'weekday', 'raining']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "condition should be string or Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-bb29b45fa94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrunModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregressionMethodName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstationDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureInputCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitMethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-1e7514882249>\u001b[0m in \u001b[0;36mrunModel\u001b[0;34m(regressionMethodName, stationID, stationDataFrame, featureInputCols, normalize, splitMethod)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mstationSummaryAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstationDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'station_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstddev_pop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mstationAvg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstationSummaryAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg(label)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'station_id'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstationID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mstationSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstationSummaryAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum(label)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'station_id'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstationID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mstationStd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstationSummaryAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stddec_pop(label)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'station_id'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstationID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/server/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: condition should be string or Column"
     ]
    }
   ],
   "source": [
    "\n",
    "runSingle = True\n",
    "test = False\n",
    "\n",
    "if runSingle == True:\n",
    "    print('run single station model')\n",
    "    t0 = time.time()\n",
    "    #select single station for initial run\n",
    "    station = sampleStationList[67]\n",
    "\n",
    "    labelLinkFunction = 'none'\n",
    "\n",
    "    #create dataframe for selected station and \n",
    "    stationDataFrame = createStationDataFrame(station,labelLinkFunction=labelLinkFunction) \n",
    "\n",
    "    #create list of columns to include in feature vector\n",
    "    ignoreColumnList= ['datetime', 'station_id', 'total_precip','cloud_cover','dayOfWeek','totalDemand', 'totalSupply','label']\n",
    "\n",
    "    featureInputCols = createfeatureInputCols(stationDataFrame, ignoreColumnList)   \n",
    "    regressionMethodName = \"rf\"\n",
    "    normalize = False\n",
    "\n",
    "    runModel(regressionMethodName,station,stationDataFrame, featureInputCols, normalize, splitMethod = \"dates\")    \n",
    "    print(e)\n",
    "    print(time.time()-t0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model on all stations in the sample\n",
    "test = False\n",
    "labelLinkFunction = 'none'\n",
    "ignoreColumnList= ['datetime', 'station_id', 'total_precip','cloud_cover','dayOfWeek','totalDemand', 'totalSupply','label']\n",
    "regressionMethodName = \"rf\"\n",
    "normalize = True\n",
    "\n",
    "normalizeList = [False]\n",
    "modelList = ['rf']\n",
    "\n",
    "for s in sampleStationList:\n",
    "    sDF = createStationDataFrame(s,labelLinkFunction=labelLinkFunction) \n",
    "    featureInputCols = createfeatureInputCols(sDF, ignoreColumnList)   \n",
    "    for m in modelList:\n",
    "        runModel(m,s,sDF, featureInputCols, normalize) \n",
    "    print(time.time()-t0)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "linkFunctionLanel = 'none'\n",
    "\n",
    "285\n",
    "\n",
    "rf False\n",
    "\n",
    "['temp', 'humidity', 'year', 'month', 'hourOfDay', 'weekday', 'raining']\n",
    "\n",
    "RMSE, R2, and variance explained on test data =  1.397,  0.587,  0.587\n",
    "\n",
    "\n",
    "\n",
    "#predictions.select(featureName).collect()\n",
    "\n",
    "#RF with log link function R2 approaches 0.4 with unnormalized features :RMSE and R2 on test data =  0.526,  0.476\n",
    "#RF with log link function and normalized data: RMSE and R2 on test data =  0.649,  0.183\n",
    "#added raining as boolean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "plotPredictedvsActual(predictions)\n",
    "print(time.time()-t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.stages[2])\n",
    "model.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTreesList = [10]\n",
    "# small number of trees is suboptimal based on initial grid search\n",
    "#numTreesList = [25, 35, 42, 50]\n",
    "subsamplingRateList = [1.0]\n",
    "maxDepthList = [0]\n",
    "#optimumm maxDepth ~10 based on initial grid search\n",
    "#maxDepthList = [10]\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "rfHyperParameterSerach(numTreesList,subsamplingRateList,maxDepthList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
