{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, avg, udf, to_timestamp, date_trunc\n",
    "from pyspark.sql.functions import year, month, hour, dayofweek\n",
    "from pyspark.sql.functions import round, concat, col, lit\n",
    "from pyspark.sql.functions import log1p\n",
    "\n",
    "from pyspark.sql.types import FloatType, StructType, IntegerType, StringType, DoubleType, StructField, TimestampType, DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, Normalizer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import random\n",
    "\n",
    "spark1 = SparkSession.builder.appName(\"SingleStationRF\").getOrCreate()\n",
    "\n",
    "import datetime as dt\n",
    "print(\"modules imported\")\n",
    "\n",
    "randomSeed = 1984\n",
    "\n",
    "pathWeather = \"/users/sajudson/Dropbox/WPI/DS504/project/weather/\"\n",
    "pathData = \"/users/sajudson/Dropbox/WPI/DS504/project/data/\"\n",
    "pathFigure = \"/users/sajudson/Dropbox/WPI/DS504/project/figures/\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sample Station Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.271540880203247\n"
     ]
    }
   ],
   "source": [
    "#SELECT WHICH CITY TO USE FOR ANALYSIS \n",
    "city = \"NYC\"\n",
    "#city = \"JC\"\n",
    "\n",
    "tripSampleFraction = .20\n",
    "stationSampleFraction = .20\n",
    "\n",
    "sample = str(int(tripSampleFraction*100))\n",
    "\n",
    "\n",
    "t0= time.time()\n",
    "\n",
    "filenameBF = \"citibike\"+city+\"bf3_sample_\"+sample\n",
    "filenameOutput = filenameBF+\".csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "sampleStationDataFeatureSchema = StructType([StructField('datetime', TimestampType(), False),\n",
    "                              StructField('station_id', IntegerType(), False),\n",
    "                              StructField('totalDemand', IntegerType(), False),\n",
    "                              StructField('totalSupply', IntegerType(), False)    \n",
    "                              ])\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "sampleStationData = spark1.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(sampleStationDataFeatureSchema) \\\n",
    "  .load(pathData+filenameOutput)\n",
    "\n",
    "print(time.time()-t0)\n",
    "# this operation takes ~0.27seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing and debugging only\n",
    "#t0= time.time()\n",
    "#sampleStationData.describe().show()\n",
    "#sampleStationData.show()\n",
    "#print(time.time()-t0)\n",
    "\n",
    "# this operation takes ~3.4 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weather Features Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherFeatures = \"NYC\"+'weatherFeatures'\n",
    "weather_file_type = 'csv'\n",
    "weatherFilename = weatherFeatures + \".\"+weather_file_type\n",
    "\n",
    "weatherFeatureSchema = StructType([StructField('temp', DoubleType(), False),\n",
    "                            StructField('humidity', DoubleType(), True),\n",
    "                            StructField('total_precip', DoubleType(), True),\n",
    "                            StructField('cloud_cover', DoubleType(), True),                 \n",
    "                            StructField('datetime', TimestampType(), True)\n",
    "                           ])\n",
    "\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "weatherFeatures = spark1.read.format(weather_file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(weatherFeatureSchema) \\\n",
    "  .load(pathWeather+weatherFilename)\n",
    "\n",
    "# for testing and debugging only\n",
    "#weatherFeatures.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate List of Stations included in Sample Station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleStationList = sampleStationData.select(\"station_id\").distinct().orderBy(\"station_id\").rdd.map(lambda row : row[0]).collect() \n",
    "\n",
    "# for testing and debugging only\n",
    "#print(sampleStationList)\n",
    "#Expected Output \n",
    "#[3183, 3184, 3188, 3193, 3199, 3200, 3207, 3209, 3213, 3214, 3220, 3225, 3281, 3483]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect sample station list data set\n",
    "\n",
    "inspectStationSample = False\n",
    "if inspectStationSample == True:\n",
    "      \n",
    "    stationRecordAverage =0\n",
    "    stationRecordMax =0\n",
    "    stationRecordMin =999999999\n",
    "    stationRecordCount = []\n",
    "    stationCount = len(sampleStationList)\n",
    "    for i in  range(0,stationCount):\n",
    "        s = sampleStationList[i]            \n",
    "        sdf = sampleStationData.filter(sampleStationData.station_id == s) #.orderBy('datetime')\n",
    "        stationRecordCount.append(sdf.count())\n",
    "        stationRecordAverage = stationRecordAverage + stationRecordCount[i]/stationCount\n",
    "        if stationRecordMax < stationRecordCount[i]:\n",
    "            stationRecordMax =stationRecordCount[i]\n",
    "        if stationRecordMin > stationRecordCount[i]:\n",
    "            stationRecordMin =stationRecordCount[i]\n",
    "        print(\"station_id =\",s, \"records = \", stationRecordCount[i])\n",
    "        print(sdf.show(1))\n",
    "    print(stationRecordAverage)\n",
    "    print(stationRecordAverage/(3*8760))\n",
    "    print(stationRecordMin/(3*8760))\n",
    "    print(stationRecordMax/(3*8760))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Station Level Data Set\n",
    "- Filter based on station selected\n",
    "- Merge with weather data\n",
    "- Fill hours without any trip data (i.e., demand or supply is null) with zeroes\n",
    "- Create date based features in station data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id = 285\n",
      "9112\n",
      "rows in dataframe 26304\n",
      "2.9363646507263184\n",
      "3.4867498874664307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime: timestamp, temp: double, humidity: double, total_precip: double, cloud_cover: double, station_id: int, totalDemand: int, totalSupply: int, year: int, month: int, hourOfDay: int, dayOfWeek: int, weekday: boolean, logDemand: double, logSupply: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.029751777648926\n",
      "['datetime', 'temp', 'humidity', 'total_precip', 'cloud_cover', 'station_id', 'totalDemand', 'totalSupply', 'year', 'month', 'hourOfDay', 'dayOfWeek', 'weekday', 'logDemand', 'logSupply']\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#select single station for initial run\n",
    "station = 285 #sampleStationList[67]\n",
    "print(\"station_id =\",station)\n",
    "#tripSampleFraction = 1.0\n",
    "bf_station = sampleStationData.filter(sampleStationData.station_id == station)\n",
    "#bf_station = bf_station.sample(False, tripSampleFraction, randomSeed)\n",
    "    \n",
    "print(bf_station.count())\n",
    "\n",
    "\n",
    "#left join includes all intervals in weather file in output - then fill supply and demand nulls with zeroes\n",
    "#right joing only includes intervals with supply or demand \n",
    "bf_station = weatherFeatures.join(bf_station, ['datetime'],how = \"left\")\n",
    "bf_station = bf_station.fillna({'totalDemand':'0','totalSupply':'0'})\n",
    "\n",
    "#bf_station.show()\n",
    "print(\"rows in dataframe\",bf_station.count())\n",
    "print(time.time()-t0)\n",
    "    \n",
    "#print(sampleStationList)\n",
    "\n",
    "# year month and hour are redundent with metblue data fields\n",
    "bf_station = bf_station.withColumn(\"year\", year(bf_station.datetime).cast(\"integer\"))\n",
    "bf_station = bf_station.withColumn(\"month\", month(bf_station.datetime).cast(\"integer\"))\n",
    "#bf_station = bf_station.withColumn(\"date\", date_trunc(\"day\", bf_station.datetime))\n",
    "\n",
    "@udf('boolean')\n",
    "def ifWeekday(dow):\n",
    "    if dow > 5.0: return False\n",
    "    else: return (True)\n",
    "\n",
    "bf_station = bf_station.withColumn(\"hourOfDay\", hour(bf_station.datetime).cast('integer'))\n",
    "bf_station = bf_station.withColumn(\"dayOfWeek\", dayofweek(bf_station.datetime).cast(\"integer\"))\n",
    "bf_station = bf_station.na.drop(how=\"any\", subset=['dayOfWeek','hourOfDay'])\n",
    "bf_station = bf_station.withColumn(\"weekday\", ifWeekday(bf_station.dayOfWeek))\n",
    "\n",
    "#Label y\n",
    "linkFunction = \"log1p\"\n",
    "#linkFunction = \"none\"\n",
    "if linkFunction == \"log1p\":\n",
    "    bf_station = bf_station.withColumn(\"logDemand\", log1p(bf_station.totalDemand))\n",
    "    bf_station = bf_station.withColumn(\"logSupply\", log1p(bf_station.totalSupply))\n",
    "    \n",
    "else:\n",
    "    bf_station = bf_station.withColumn(\"label\", bf_station.totalDemand)\n",
    "\n",
    "print(time.time()-t0)\n",
    "\n",
    "bf_station.cache()\n",
    "#bf_station.show()\n",
    "print(time.time()-t0) \n",
    "#bf_station.describe().show()\n",
    "print(bf_station.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10967 744\n"
     ]
    }
   ],
   "source": [
    "trainingDates = ['2016-10-01 00:00:00',\n",
    "                 '2017-9-30 23:59:59']\n",
    "testDates = ['2017-10-01 00:00:00',\n",
    "             '2017-10-31 23:59:59']\n",
    "dates = [trainingDates, testDates]\n",
    "\n",
    "def timeSeriesTestTrain(df,dates):\n",
    "    train = df.where(df.datetime.between(dates[0][0],dates[0][1]))\n",
    "    test = df.where(df.datetime.between(dates[1][0],dates[1][1]))\n",
    "    return (train,test)\n",
    "\n",
    "test, train = timeSeriesTestTrain(bf_station,dates)\n",
    "print(test.count(), train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = to_timestamp('2016-10-01 00:00')\n",
    "endDate = to_timestamp('2016-10-01 00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'to_timestamp(`2016-10-01 00:00`)'>\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingDates = ['2016-12-01 00:00:00',\n",
    "                 '2017-9-30 23:59:59']\n",
    "    \n",
    "testDates = ['2017-10-01 00:00:00',\n",
    "             '2017-10-31 23:59:59']\n",
    "    \n",
    "dates = {'train':trainingDates, 'test':testDates}\n",
    "\n",
    "def timeSeriesTestTrain(df,dates):\n",
    "    train = df.where(df.datetime.between(dates['train'][0],dates['train'][1]))\n",
    "    test = df.where(df.datetime.between(dates['test'][0],dates['test'][1]))\n",
    "    return (train,test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9503 744\n"
     ]
    }
   ],
   "source": [
    "test, train = timeSeriesTestTrain(bf_station,dates)\n",
    "print(test.count(), train.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 84\n"
     ]
    }
   ],
   "source": [
    "j = int(np.random.random() *100)\n",
    "e =0\n",
    "for i in range(0,j):\n",
    "    e += 1\n",
    "\n",
    "print(j,e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1541221256.1035452"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
